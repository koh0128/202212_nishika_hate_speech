{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2366,"status":"ok","timestamp":1668158697381,"user":{"displayName":"å¤§æ©‹å¹¸è¨˜","userId":"14481296101370409019"},"user_tz":-540},"id":"mhj-dILzd2wX","outputId":"cf31fdba-a48d-47bc-fcf2-f561603581ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# ãƒ‰ãƒ©ã‚¤ãƒ–ã‚’ãƒžã‚¦ãƒ³ãƒˆ\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"lq4EcF-5d2pr","executionInfo":{"status":"ok","timestamp":1668158697382,"user_tz":-540,"elapsed":4,"user":{"displayName":"å¤§æ©‹å¹¸è¨˜","userId":"14481296101370409019"}}},"outputs":[],"source":["# è‡ªèº«ã®Colabç’°å¢ƒã«ã‚ˆã£ã¦ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å¤‰æ›´ã—ã¦ä¸‹ã•ã„\n","import os\n","os.chdir(\"/content/drive/MyDrive/ColabNotebooks/nishika/hate_speech\")"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18566,"status":"ok","timestamp":1668158715944,"user":{"displayName":"å¤§æ©‹å¹¸è¨˜","userId":"14481296101370409019"},"user_tz":-540},"id":"WlnqZgDXdRIY","outputId":"b72f4731-abec-477d-9b2a-3f617c3bfd4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (2.6.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: dill<0.3.6 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.1)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.10.1)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.10.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.3)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.1.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.13.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.13)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (22.1.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.1.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.8.1)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.3)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.1.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.10.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.24.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece==0.1.91 in /usr/local/lib/python3.7/dist-packages (0.1.91)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: neologdn in /usr/local/lib/python3.7/dist-packages (0.5.1)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: mojimoji in /usr/local/lib/python3.7/dist-packages (0.0.12)\n"]}],"source":["!pip install datasets\n","!pip install transformers\n","!pip install sentencepiece==0.1.91\n","!pip install neologdn\n","!pip install mojimoji"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"zLzrgn7TK7af","executionInfo":{"status":"ok","timestamp":1668158720385,"user_tz":-540,"elapsed":4444,"user":{"displayName":"å¤§æ©‹å¹¸è¨˜","userId":"14481296101370409019"}}},"outputs":[],"source":["import os\n","import datasets\n","import numpy as np\n","import pandas as pd\n","from datasets import load_metric\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    T5Tokenizer,\n","    Trainer,\n","    TrainingArguments,\n",")\n","import neologdn\n","import mojimoji"]},{"cell_type":"code","source":["def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True, max_length=77)\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","def preprocessing(df):\n","\n","    preprocessed_df = df.copy()\n","    preprocessed_df['text'] = preprocessed_df['text'].apply(lambda x:x.replace('\\n', ''))\n","    preprocessed_df['text'] = preprocessed_df['text'].apply(lambda x:x.replace(' ', ''))\n","    preprocessed_df['text'] = preprocessed_df['text'].apply(lambda x:x.replace('ã€€', ''))\n","    preprocessed_df['text'] = preprocessed_df['text'].apply(lambda x:neologdn.normalize(x, repeat=3))\n","\n","    return preprocessed_df\n","\n","if __name__ == \"__main__\":\n","\n","    os.environ[\"WANDB_DISABLED\"] = \"true\"\n","    metric = load_metric(\"f1\")\n","\n","    df = pd.read_csv(\"/content/drive/MyDrive/input/nishika/hate_speech/train.csv\")\n","    df = preprocessing(df)\n","\n","    tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    test = pd.read_csv(\"/content/drive/MyDrive/input/nishika/hate_speech/test.csv\")\n","    test = preprocessing(test)\n","\n","    test_dataset = datasets.Dataset.from_pandas(test[[\"text\"]])\n","    test_tokenized = test_dataset.map(preprocess_function, batched=True)\n","\n","    train_dataset = datasets.Dataset.from_pandas(df[[\"text\", \"label\"]])\n","    train_tokenized = train_dataset.map(preprocess_function, batched=True)\n","\n","    val_dataset = datasets.Dataset.from_pandas(df[[\"text\", \"label\"]])\n","    val_tokenized = val_dataset.map(preprocess_function, batched=True)\n","\n","    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","    model = AutoModelForSequenceClassification.from_pretrained(\n","        \"rinna/japanese-gpt2-medium\", num_labels=2\n","    )\n","    model.config.pad_token_id = model.config.eos_token_id\n","\n","    fold_id = 0\n","\n","    training_args = TrainingArguments(\n","        output_dir=f\"../tmp/results{fold_id}\",\n","        learning_rate=4e-5,\n","        per_device_train_batch_size=4,\n","        per_device_eval_batch_size=64,\n","        num_train_epochs=3,\n","        weight_decay=0.01,\n","        evaluation_strategy=\"steps\",\n","        eval_steps=250,\n","        load_best_model_at_end=True,\n","        save_steps=1000,\n","        gradient_accumulation_steps=3,\n","        save_total_limit=3,\n","        fp16=True,\n","    )\n","\n","    trainer = Trainer(\n","        model=model,\n","        args=training_args,\n","        train_dataset=train_tokenized,\n","        eval_dataset=val_tokenized,\n","        tokenizer=tokenizer,\n","        data_collator=data_collator,\n","        compute_metrics=compute_metrics,\n","    )\n","\n","    trainer.train()\n","\n","    oof_results = trainer.predict(test_dataset=val_tokenized)\n","    np.save(f\"oof_prediction{fold_id}\", oof_results.predictions)\n","\n","    results = trainer.predict(test_dataset=test_tokenized)\n","    np.save(f\"test_prediction{fold_id}\", results.predictions)\n","\n","    test[\"isFake\"] = np.argmax(results.predictions, axis=-1)\n","    test[[\"id\", \"isFake\"]].to_csv(\"submission.csv\", index=False)"],"metadata":{"id":"jFPK-cpRc8Wy","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8de09ae72a2741c5b9915c4818ca5bc1","b3e08a7f90ea445396fca89bae2f4d18","5dbd2747e7cf4a68a66fb4f7c395af93","3843a5d2d35f4021a1c27f646fc89219","c2cfe2a518f04b25b69408871f7a06fc","9b01f70974cb4ce5ab0aa3fb9533c9b6","aa908149f14e499eb0c7426b2cd04cea","9f2dd745cbec48cb99360eb4f55ddc5a","69a6b20eca6647a087891b7541a39f5e","31f78c6f74834029b065d7607449d961","3dabf8a5efb843918b9c1ac573fccbd8","607bbd1e6c284ce3b74da489f9c3efd1","697d0fa5a7ce4c9aaa513429339e9553","6a1ab7b5ed7641aa89218e6e8947ed03","54d3074a645b43cbbe894776c01054c2","938f21c4186545d98704898fb9cf67a3","1baef6b581bc474da92fe0ea7fa702d1","6e5d5c34473a4c60844f7340117c1f8a","1f1547088e0a473fa1f997b0899bd0f5","5f4bbc2922ef4dfb81814950e119e009","0e7a89ae6a154dba9818cf0d42bb30b2","bd2755cca9d14d7bb2f0182c0aee08bd","48d6dea9bcc24a8da97511e39c419ddb","734a7cd49ce648ea87a24aa94095d097","f31565a6254242c1911f7664f724882c","a5987928eebe43f085ebc08ba0770a79","84bb84e193b14fe8a8e27c3558e6b1b3","72f04577e8f946eab344debd69ce2fcf","36adea7d84d34fc1b7a80b1421fac5e1","de72a03db8e14e30a90bd7cae5a63a4e","dd47c82f61c14bf8ab3ab44497e96c02","533bd5865b79458b8a519011e938048b","c2b65ee9400c45fe828889eb001129a7"]},"executionInfo":{"status":"ok","timestamp":1668159494837,"user_tz":-540,"elapsed":774459,"user":{"displayName":"å¤§æ©‹å¹¸è¨˜","userId":"14481296101370409019"}},"outputId":"85baf7fe-1ea4-48ff-cf13-69e201227965"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8de09ae72a2741c5b9915c4818ca5bc1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/6 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607bbd1e6c284ce3b74da489f9c3efd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/6 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48d6dea9bcc24a8da97511e39c419ddb"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at rinna/japanese-gpt2-medium were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n","- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at rinna/japanese-gpt2-medium and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 5256\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 12\n","  Gradient Accumulation steps = 3\n","  Total optimization steps = 1314\n","  Number of trainable parameters = 336130048\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1314' max='1314' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1314/1314 12:00, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>250</td>\n","      <td>No log</td>\n","      <td>0.067062</td>\n","      <td>0.729045</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.128800</td>\n","      <td>0.024082</td>\n","      <td>0.914186</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.128800</td>\n","      <td>0.020704</td>\n","      <td>0.945205</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.027100</td>\n","      <td>0.002090</td>\n","      <td>0.996743</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.027100</td>\n","      <td>0.000586</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 5256\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 5256\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 5256\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 5256\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results0/checkpoint-1000\n","Configuration saved in ../tmp/results0/checkpoint-1000/config.json\n","Model weights saved in ../tmp/results0/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results0/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results0/checkpoint-1000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 5256\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ../tmp/results0/checkpoint-1000 (score: 0.0020900091622024775).\n","The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 5256\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 3223\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}]},{"cell_type":"code","source":["import os\n","import datasets\n","import numpy as np\n","import pandas as pd\n","from datasets import load_metric\n","from sklearn.model_selection import StratifiedKFold\n","from transformers import (\n","    AutoModelForSequenceClassification,\n","    DataCollatorWithPadding,\n","    T5Tokenizer,\n","    Trainer,\n","    TrainingArguments,\n",")\n","\n","\n","def preprocess_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True, max_length=77)\n","\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)\n","\n","def preprocessing(df):\n","\n","    preprocessed_df = df.copy()\n","    preprocessed_df['text'] = preprocessed_df['text'].apply(lambda x:x.replace('\\n', ''))\n","    preprocessed_df['text'] = preprocessed_df['text'].apply(lambda x:x.replace(' ', ''))\n","    preprocessed_df['text'] = preprocessed_df['text'].apply(lambda x:x.replace('ã€€', ''))\n","    preprocessed_df['text'] = preprocessed_df['text'].apply(lambda x:neologdn.normalize(x, repeat=3))\n","\n","    return preprocessed_df\n","\n","\n","if __name__ == \"__main__\":\n","\n","    os.environ[\"WANDB_DISABLED\"] = \"true\"\n","    metric = load_metric(\"f1\")\n","\n","    df = pd.read_csv(\"/content/drive/MyDrive/input/nishika/hate_speech/train.csv\")\n","    df = preprocessing(df)\n","    df.text = [\n","        data[: data.find(\"ã€‚\")] + \"ã€‚[SEP]\" + data[data.find(\"ã€‚\") + 1 :]\n","        for data in df.text\n","    ]\n","\n","    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","    for n, (train_index, val_index) in enumerate(cv.split(df, df[\"label\"])):\n","        df.loc[val_index, \"fold\"] = int(n)\n","    df[\"fold\"] = df[\"fold\"].astype(int)\n","\n","    tokenizer = T5Tokenizer.from_pretrained(\"rinna/japanese-gpt2-medium\")\n","    tokenizer.pad_token = tokenizer.eos_token\n","\n","    test = pd.read_csv(\"/content/drive/MyDrive/input/nishika/hate_speech/test.csv\")\n","    test = preprocessing(test)\n","    test.text = [\n","        data[: data.find(\"ã€‚\")] + \"ã€‚[SEP]\" + data[data.find(\"ã€‚\") + 1 :]\n","        for data in test.text\n","    ]\n","    test_dataset = datasets.Dataset.from_pandas(test[[\"text\"]])\n","    test_tokenized = test_dataset.map(preprocess_function, batched=True)\n","\n","    test_prediction0 = np.load(\n","        \"test_prediction0.npy\"\n","    )\n","    test[\"pred0\"] = test_prediction0[:, 0]\n","    test[\"pred1\"] = test_prediction0[:, 1]\n","    test[\"label\"] = (test[\"pred0\"] < test[\"pred1\"]).astype(int)\n","    test_filter = test.query(\"pred0 > 3 or pred0 < -3\")[[\"id\", \"label\", \"text\"]]\n","\n","    for fold_id in range(5):\n","\n","        if fold_id in (0, 1, 2, 3, 4):\n","\n","            train_dataset = datasets.Dataset.from_pandas(\n","                pd.concat([df.query(f\"fold != {fold_id}\"), test_filter]).reset_index(\n","                    drop=True\n","                )[[\"text\", \"label\"]]\n","            )\n","            train_tokenized = train_dataset.map(preprocess_function, batched=True)\n","\n","            val_dataset = datasets.Dataset.from_pandas(\n","                df.query(f\"fold == {fold_id}\")[[\"text\", \"label\"]]\n","            )\n","            val_tokenized = val_dataset.map(preprocess_function, batched=True)\n","\n","            data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","            model = AutoModelForSequenceClassification.from_pretrained(\n","                \"rinna/japanese-gpt2-medium\", num_labels=2\n","            )\n","            model.config.pad_token_id = model.config.eos_token_id\n","\n","            training_args = TrainingArguments(\n","                output_dir=f\"../tmp/results{fold_id}\",\n","                learning_rate=4e-5,\n","                per_device_train_batch_size=4,\n","                per_device_eval_batch_size=64,\n","                num_train_epochs=6,\n","                weight_decay=0.01,\n","                evaluation_strategy=\"steps\",\n","                eval_steps=250,\n","                load_best_model_at_end=True,\n","                save_steps=1000,\n","                gradient_accumulation_steps=3,\n","                save_total_limit=3,\n","                fp16=True,\n","            )\n","\n","            trainer = Trainer(\n","                model=model,\n","                args=training_args,\n","                train_dataset=train_tokenized,\n","                eval_dataset=val_tokenized,\n","                tokenizer=tokenizer,\n","                data_collator=data_collator,\n","                compute_metrics=compute_metrics,\n","            )\n","\n","            trainer.train()\n","\n","            oof_results = trainer.predict(test_dataset=val_tokenized)\n","            np.save(f\"oof_prediction{fold_id}\", oof_results.predictions)\n","            oof = df.query(f\"fold == {fold_id}\").copy()\n","            oof[\"pred0\"] = oof_results.predictions[:, 0]\n","            oof[\"pred1\"] = oof_results.predictions[:, 1]\n","            oof.to_csv(f\"oof{fold_id}.csv\", index=False)\n","\n","            results = trainer.predict(test_dataset=test_tokenized)\n","            np.save(f\"test_prediction{fold_id}\", results.predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["76b11c90c9fc491cbd4675fa8e75b4f6","3d814133feb44f87a83a8f4210fa6b11","593fbb48c56b4495bb5700119a58ac46","5738ba72315d4a5baa0765f5c7d49cdc","254b5afaa1144c63a9d5da190e947b8b","b1500f82d55840c5b920cf5ecc0a9566","556ec94d0f6348378ec90074d5492084","00681fc7e293424d8dbf739d2c37af70","254a7d26215c426a9e4f22b4aeb25db0","412a62a83c184422b8bfb629ba05b305","bda988f7fbd041c5bc75172b0b5a16a1","485bdc22695b4602a2c44e9eae496fc4","37f3de8aa0964b23bcbbfff1daa67040","b85bfe1925654bb2918537615ab2ef7d","cf5cc4256653429784cbd9de0e16ac30","c5947e7d26294779973a8be830df7561","7f2e903b4df64feb9d71ef18583ae0b1","a52d43c746f744359d40eac33dde82c5","7d1f4cb770374086a1ed41e7f012f766","db6d023996db4d5a84dce9fbfe8bbc5a","f5a5ec90df884c3d974dd131c39c1814","1fac7d2e3d574bf18d171e85ccf1c72c","dfa1cca173d447c1899bfd96d5c89391","43bc8e3128814fef98f34d2af8ec1352","f0b6e50f5b6540e59016248b344cf0d3","b5efc16760e746138914f4a873c40554","9b63f33ef25244b7b8e5233596c15246","0f359969fa62499f8d597cc5bda29d4d","47c4eb84d2674f398285426766031f82","b65d6e42cdcd46fe9e0834ba241de6b2","6b0073fc960d495a8c38dd501cc295de","3925177bb09c4fb3a8de8a1f1822b262","600926c0f2c0456d965761369574dbaf","cef4fa8e86b1418d8389fa1626916bf4","b818bdee18f1482eb532ede4a8658f69","d7ded6850b49496782c3f41bd6c0d91d","1dae50a35f7e4f869a1acb9b37b87972","9a7a52584d804ade96a568be7d6e6221","e394a2d95b2b4d5a905e050983e38416","88a48495ba20489d911e9e3cb64eb78b","7947126e9b9948d18467727cfc4d6562","b1fa991106ea41438a4b8c31bf260e58","181bd894ce7a4c2ab6a9c0eda613ea19","711fd0b6a7704f4f9729e0e1b3ebb0f3","39f873659b8c4e6b8b8f26096c892f3c","c07ead273e904c63a70e770539dea2ce","8dff4686cfde47e4ba9ba93404489d11","94639c6b0f234941ad6c8799814500f6","9d48ca5f4de44a328c20ae73e35a350d","c8ad7c7792a3472081982402dd478c33","43544eb5e3ba40cc9c2feaecf70deac6","faa39562cb3e4bb2804930afd6368c2c","6c75f6fae6f04340869487fb7adb4659","d85efa4c4cd747aaa6f5d4809d6c517b","f81398f080794e2c846522ca307f308c","bdd18eb0417446e7bc4dace5bcfc518a","41cbf380ab5342a09e92795ee805a730","647d12b2e39240f3bd08ea3e6bb92666","f2ccf14656d74e0b8b0631ba7fe87623","9d215692b2cf4c25921b96ce91c57cc1","d72bbb851919447b96373139f40107ab","37cb3a06fb38407a81538d8b30eb1111","f0225dcc0c954b7d9cb9bffe661f1478","5b7052614d444cebaf39eb41f276018f","1f70706a10fb4cebb0588242df422c6a","1441a62392044d8eb6fc5f238df536cd","ee07bedbcc00456a8d40a9215d3117d4","0ddaffedee88407e972b1caf82655cc0","e613314a96244a199c3943ee714bfea3","788a5532f78f413a84005ae794a0162f","56968a761cd24c94ada12ebf951e26ce","86efacd0003c4eaeb67b6b6ac0b10210","373db82d8e4d467bbc4554c2cddf52bb","f29d77eeeceb4302b03e59299fe2eadf","ff1fc0ba199e4911856f11ebd819f2a2","851d2ac043174ca184b5e7757bf741e5","e9bbf888b68a4804b58c11c46e85b238","e3c63927ea404b63bb770bacb0b18e00","a0995a4f86a5459e8aad531fbb425e2d","94233c9009554958b9628a711c57c83c","fc86cc9aa0774d79aef8e260636d2013","1b112b9bc1814f8394ded9c9f4f5c56e","8e6ca2e1575343b4b3de5a2d5db37b26","9de91bf760314914a463f49410212f77","8dcae2c05ccc46469a3da3d85222dfd5","89b873bf17374cb58275e528d06d92ff","bcd2c879307947f093b79115cb9b3384","0f50419d81364f4382ced9b9e56a13a8","b1496965b5a740d3a38f4f057288e408","51fad95e4ffb4076bc7ac021b35fbc37","1a091fd4cce24b41a6a3d6ce0952e845","9177941bba5b475b9c3df3f2d7de25b6","d614a140bf2f465a9700b9562b32303b","5bd9798c3776400ea63d349ef12991a3","d486053cf3d94531b9f5473c1244b35f","3d873052513e460ba4d59024b67ced21","bd7b0530da074df6ae11070cacb9b5aa","66432dc2ed5d43fda6a6f3b0490e5f01","063f4b6978904ad9a2101de9d5135557","a958737b22db4d37a86ad0bb7e92a529","24c36c61a5774661ae4ca1acfdce82f0","c35c41a733fd4824a9c3e8bab3df31ee","b4087c6061714f08a9c6ed059e078216","7668063bdb1347de865284495a5ff0cb","c6bbdce098eb49968541f0f7952f3f77","2344fb5f20f94305bbeda9549d05c092","0d68fd48c8034fa0b683ac80eb93af82","31801d80f32e4b7db066a49524011300","246b327472d54d52b9ae1a432671e75c","c2b42a7a5c4e49208c0b741db5aef4b9","90e127ea0e8c4946a927b6ef98888ab5","2bc4d56f3027484c8ce1619e3db31fab","d15ca3db1a5448699617a9991f96acd8","b77204317cf442a09201fa171d695c91","bc715eb650234921b168555f40ed0f56","cf08a9aca4bd4148873ba0450b535e4e","97805e70852845c68126a2a07c2c1370","6b8d86dc8354438b916536385fea0ce0","56d9239e3107485e979e7df1023a3c8a","0931b0def45a4bd98f05d60628a5d345","3391c373c5c24bda9776e2de3947e280"]},"id":"6rylZTlBNu_o","outputId":"e48878c3-728d-4de8-8710-376835558805","executionInfo":{"status":"ok","timestamp":1668169105803,"user_tz":-540,"elapsed":4954260,"user":{"displayName":"å¤§æ©‹å¹¸è¨˜","userId":"14481296101370409019"}}},"execution_count":6,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading file spiece.model from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/spiece.model\n","loading file added_tokens.json from cache at None\n","loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/special_tokens_map.json\n","loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/tokenizer_config.json\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76b11c90c9fc491cbd4675fa8e75b4f6","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/4 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"485bdc22695b4602a2c44e9eae496fc4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/7 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dfa1cca173d447c1899bfd96d5c89391","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"rinna/japanese-gpt2-medium\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 1,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 1024,\n","  \"n_head\": 16,\n","  \"n_inner\": 4096,\n","  \"n_layer\": 24,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.24.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/pytorch_model.bin\n","Some weights of the model checkpoint at rinna/japanese-gpt2-medium were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n","- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at rinna/japanese-gpt2-medium and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 6952\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 12\n","  Gradient Accumulation steps = 3\n","  Total optimization steps = 3474\n","  Number of trainable parameters = 336130048\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3474' max='3474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3474/3474 31:19, Epoch 5/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>250</td>\n","      <td>No log</td>\n","      <td>0.167771</td>\n","      <td>0.560000</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.123800</td>\n","      <td>0.121989</td>\n","      <td>0.695035</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.123800</td>\n","      <td>0.111998</td>\n","      <td>0.693548</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.035500</td>\n","      <td>0.162048</td>\n","      <td>0.641509</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.035500</td>\n","      <td>0.140381</td>\n","      <td>0.700000</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.012500</td>\n","      <td>0.236805</td>\n","      <td>0.681159</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>0.012500</td>\n","      <td>0.208827</td>\n","      <td>0.714286</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.001200</td>\n","      <td>0.226682</td>\n","      <td>0.719298</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.001200</td>\n","      <td>0.237816</td>\n","      <td>0.716667</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.000100</td>\n","      <td>0.245062</td>\n","      <td>0.716667</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>0.000100</td>\n","      <td>0.250548</td>\n","      <td>0.728814</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.000000</td>\n","      <td>0.254376</td>\n","      <td>0.728814</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>0.000000</td>\n","      <td>0.256459</td>\n","      <td>0.728814</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results0/checkpoint-1000\n","Configuration saved in ../tmp/results0/checkpoint-1000/config.json\n","Model weights saved in ../tmp/results0/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results0/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results0/checkpoint-1000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results0/checkpoint-2000\n","Configuration saved in ../tmp/results0/checkpoint-2000/config.json\n","Model weights saved in ../tmp/results0/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results0/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results0/checkpoint-2000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results0/checkpoint-3000\n","Configuration saved in ../tmp/results0/checkpoint-3000/config.json\n","Model weights saved in ../tmp/results0/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results0/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results0/checkpoint-3000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1052\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ../tmp/results0/checkpoint-1000 (score: 0.16204802691936493).\n","The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1052\n","  Batch size = 64\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 3223\n","  Batch size = 64\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cef4fa8e86b1418d8389fa1626916bf4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/7 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"39f873659b8c4e6b8b8f26096c892f3c","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"rinna/japanese-gpt2-medium\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 1,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 1024,\n","  \"n_head\": 16,\n","  \"n_inner\": 4096,\n","  \"n_layer\": 24,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.24.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/pytorch_model.bin\n","Some weights of the model checkpoint at rinna/japanese-gpt2-medium were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n","- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at rinna/japanese-gpt2-medium and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 6953\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 12\n","  Gradient Accumulation steps = 3\n","  Total optimization steps = 3474\n","  Number of trainable parameters = 336130048\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='3474' max='3474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3474/3474 31:57, Epoch 5/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>250</td>\n","      <td>No log</td>\n","      <td>0.152244</td>\n","      <td>0.596026</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.111700</td>\n","      <td>0.149329</td>\n","      <td>0.619469</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.111700</td>\n","      <td>0.164694</td>\n","      <td>0.641509</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.032000</td>\n","      <td>0.178363</td>\n","      <td>0.677686</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.032000</td>\n","      <td>0.152401</td>\n","      <td>0.709091</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.010600</td>\n","      <td>0.210494</td>\n","      <td>0.655462</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>0.010600</td>\n","      <td>0.219692</td>\n","      <td>0.617021</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.002700</td>\n","      <td>0.217734</td>\n","      <td>0.661017</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.002700</td>\n","      <td>0.237983</td>\n","      <td>0.640777</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.001400</td>\n","      <td>0.229152</td>\n","      <td>0.678571</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>0.001400</td>\n","      <td>0.234191</td>\n","      <td>0.672727</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.000100</td>\n","      <td>0.240432</td>\n","      <td>0.654867</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>0.000100</td>\n","      <td>0.242537</td>\n","      <td>0.660714</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results1/checkpoint-1000\n","Configuration saved in ../tmp/results1/checkpoint-1000/config.json\n","Model weights saved in ../tmp/results1/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results1/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results1/checkpoint-1000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results1/checkpoint-2000\n","Configuration saved in ../tmp/results1/checkpoint-2000/config.json\n","Model weights saved in ../tmp/results1/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results1/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results1/checkpoint-2000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results1/checkpoint-3000\n","Configuration saved in ../tmp/results1/checkpoint-3000/config.json\n","Model weights saved in ../tmp/results1/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results1/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results1/checkpoint-3000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ../tmp/results1/checkpoint-1000 (score: 0.17836278676986694).\n","The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1051\n","  Batch size = 64\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 3223\n","  Batch size = 64\n"]},{"data":{"text/html":[],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bdd18eb0417446e7bc4dace5bcfc518a","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/7 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ee07bedbcc00456a8d40a9215d3117d4","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"rinna/japanese-gpt2-medium\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 1,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 1024,\n","  \"n_head\": 16,\n","  \"n_inner\": 4096,\n","  \"n_layer\": 24,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.24.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/pytorch_model.bin\n","Some weights of the model checkpoint at rinna/japanese-gpt2-medium were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n","- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at rinna/japanese-gpt2-medium and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 6953\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 12\n","  Gradient Accumulation steps = 3\n","  Total optimization steps = 3474\n","  Number of trainable parameters = 336130048\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1463' max='3474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1463/3474 13:01 < 17:56, 1.87 it/s, Epoch 2.52/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>250</td>\n","      <td>No log</td>\n","      <td>0.155081</td>\n","      <td>0.608696</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.116900</td>\n","      <td>0.108705</td>\n","      <td>0.606557</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.116900</td>\n","      <td>0.140984</td>\n","      <td>0.574713</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.031800</td>\n","      <td>0.125824</td>\n","      <td>0.627451</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.031800</td>\n","      <td>0.163904</td>\n","      <td>0.660870</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results2/checkpoint-1000\n","Configuration saved in ../tmp/results2/checkpoint-1000/config.json\n","Model weights saved in ../tmp/results2/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results2/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results2/checkpoint-1000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3474' max='3474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3474/3474 31:45, Epoch 5/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>250</td>\n","      <td>No log</td>\n","      <td>0.155081</td>\n","      <td>0.608696</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.116900</td>\n","      <td>0.108705</td>\n","      <td>0.606557</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.116900</td>\n","      <td>0.140984</td>\n","      <td>0.574713</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.031800</td>\n","      <td>0.125824</td>\n","      <td>0.627451</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.031800</td>\n","      <td>0.163904</td>\n","      <td>0.660870</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.008900</td>\n","      <td>0.197700</td>\n","      <td>0.640000</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>0.008900</td>\n","      <td>0.226582</td>\n","      <td>0.626263</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.001600</td>\n","      <td>0.312509</td>\n","      <td>0.450000</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.001600</td>\n","      <td>0.204181</td>\n","      <td>0.709091</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.001400</td>\n","      <td>0.203990</td>\n","      <td>0.720721</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>0.001400</td>\n","      <td>0.211966</td>\n","      <td>0.727273</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.000000</td>\n","      <td>0.220390</td>\n","      <td>0.666667</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>0.000000</td>\n","      <td>0.225022</td>\n","      <td>0.666667</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results2/checkpoint-2000\n","Configuration saved in ../tmp/results2/checkpoint-2000/config.json\n","Model weights saved in ../tmp/results2/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results2/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results2/checkpoint-2000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results2/checkpoint-3000\n","Configuration saved in ../tmp/results2/checkpoint-3000/config.json\n","Model weights saved in ../tmp/results2/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results2/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results2/checkpoint-3000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ../tmp/results2/checkpoint-1000 (score: 0.1258237361907959).\n","The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1051\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 3223\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/7 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3c63927ea404b63bb770bacb0b18e00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1496965b5a740d3a38f4f057288e408"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"rinna/japanese-gpt2-medium\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 1,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 1024,\n","  \"n_head\": 16,\n","  \"n_inner\": 4096,\n","  \"n_layer\": 24,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.24.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/pytorch_model.bin\n","Some weights of the model checkpoint at rinna/japanese-gpt2-medium were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n","- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at rinna/japanese-gpt2-medium and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 6953\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 12\n","  Gradient Accumulation steps = 3\n","  Total optimization steps = 3474\n","  Number of trainable parameters = 336130048\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3474' max='3474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3474/3474 31:43, Epoch 5/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>250</td>\n","      <td>No log</td>\n","      <td>0.129413</td>\n","      <td>0.538462</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.114700</td>\n","      <td>0.150983</td>\n","      <td>0.419753</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.114700</td>\n","      <td>0.141360</td>\n","      <td>0.552381</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.037900</td>\n","      <td>0.180669</td>\n","      <td>0.511111</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.037900</td>\n","      <td>0.144613</td>\n","      <td>0.634615</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.012100</td>\n","      <td>0.227448</td>\n","      <td>0.565217</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>0.012100</td>\n","      <td>0.174483</td>\n","      <td>0.632479</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.002000</td>\n","      <td>0.207165</td>\n","      <td>0.666667</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.002000</td>\n","      <td>0.187396</td>\n","      <td>0.678261</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.000400</td>\n","      <td>0.226342</td>\n","      <td>0.626263</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>0.000400</td>\n","      <td>0.225397</td>\n","      <td>0.633663</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.000300</td>\n","      <td>0.193982</td>\n","      <td>0.683761</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>0.000300</td>\n","      <td>0.196968</td>\n","      <td>0.678261</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results3/checkpoint-1000\n","Configuration saved in ../tmp/results3/checkpoint-1000/config.json\n","Model weights saved in ../tmp/results3/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results3/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results3/checkpoint-1000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results3/checkpoint-2000\n","Configuration saved in ../tmp/results3/checkpoint-2000/config.json\n","Model weights saved in ../tmp/results3/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results3/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results3/checkpoint-2000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results3/checkpoint-3000\n","Configuration saved in ../tmp/results3/checkpoint-3000/config.json\n","Model weights saved in ../tmp/results3/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results3/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results3/checkpoint-3000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ../tmp/results3/checkpoint-1000 (score: 0.18066924810409546).\n","The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1051\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 3223\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/7 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a958737b22db4d37a86ad0bb7e92a529"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/2 [00:00<?, ?ba/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90e127ea0e8c4946a927b6ef98888ab5"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/config.json\n","Model config GPT2Config {\n","  \"_name_or_path\": \"rinna/japanese-gpt2-medium\",\n","  \"activation_function\": \"gelu_new\",\n","  \"architectures\": [\n","    \"GPT2LMHeadModel\"\n","  ],\n","  \"attn_pdrop\": 0.1,\n","  \"bos_token_id\": 1,\n","  \"embd_pdrop\": 0.1,\n","  \"eos_token_id\": 2,\n","  \"gradient_checkpointing\": false,\n","  \"initializer_range\": 0.02,\n","  \"layer_norm_epsilon\": 1e-05,\n","  \"model_type\": \"gpt2\",\n","  \"n_ctx\": 1024,\n","  \"n_embd\": 1024,\n","  \"n_head\": 16,\n","  \"n_inner\": 4096,\n","  \"n_layer\": 24,\n","  \"n_positions\": 1024,\n","  \"reorder_and_upcast_attn\": false,\n","  \"resid_pdrop\": 0.1,\n","  \"scale_attn_by_inverse_layer_idx\": false,\n","  \"scale_attn_weights\": true,\n","  \"summary_activation\": null,\n","  \"summary_first_dropout\": 0.1,\n","  \"summary_proj_to_labels\": true,\n","  \"summary_type\": \"cls_index\",\n","  \"summary_use_proj\": true,\n","  \"task_specific_params\": {\n","    \"text-generation\": {\n","      \"do_sample\": true,\n","      \"max_length\": 50\n","    }\n","  },\n","  \"transformers_version\": \"4.24.0\",\n","  \"use_cache\": true,\n","  \"vocab_size\": 32000\n","}\n","\n","loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--rinna--japanese-gpt2-medium/snapshots/f464b76739c884d8b0479a0a7705b7fa71c3fd5a/pytorch_model.bin\n","Some weights of the model checkpoint at rinna/japanese-gpt2-medium were not used when initializing GPT2ForSequenceClassification: ['lm_head.weight']\n","- This IS expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing GPT2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at rinna/japanese-gpt2-medium and are newly initialized: ['score.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","Using cuda_amp half precision backend\n","The following columns in the training set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  FutureWarning,\n","***** Running training *****\n","  Num examples = 6953\n","  Num Epochs = 6\n","  Instantaneous batch size per device = 4\n","  Total train batch size (w. parallel, distributed & accumulation) = 12\n","  Gradient Accumulation steps = 3\n","  Total optimization steps = 3474\n","  Number of trainable parameters = 336130048\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='3474' max='3474' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [3474/3474 30:30, Epoch 5/6]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>250</td>\n","      <td>No log</td>\n","      <td>0.175851</td>\n","      <td>0.346667</td>\n","    </tr>\n","    <tr>\n","      <td>500</td>\n","      <td>0.120800</td>\n","      <td>0.107422</td>\n","      <td>0.711864</td>\n","    </tr>\n","    <tr>\n","      <td>750</td>\n","      <td>0.120800</td>\n","      <td>0.118656</td>\n","      <td>0.697248</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>0.036700</td>\n","      <td>0.135916</td>\n","      <td>0.666667</td>\n","    </tr>\n","    <tr>\n","      <td>1250</td>\n","      <td>0.036700</td>\n","      <td>0.176340</td>\n","      <td>0.622642</td>\n","    </tr>\n","    <tr>\n","      <td>1500</td>\n","      <td>0.009400</td>\n","      <td>0.218475</td>\n","      <td>0.600000</td>\n","    </tr>\n","    <tr>\n","      <td>1750</td>\n","      <td>0.009400</td>\n","      <td>0.208893</td>\n","      <td>0.705882</td>\n","    </tr>\n","    <tr>\n","      <td>2000</td>\n","      <td>0.001800</td>\n","      <td>0.217264</td>\n","      <td>0.660550</td>\n","    </tr>\n","    <tr>\n","      <td>2250</td>\n","      <td>0.001800</td>\n","      <td>0.229977</td>\n","      <td>0.660550</td>\n","    </tr>\n","    <tr>\n","      <td>2500</td>\n","      <td>0.000100</td>\n","      <td>0.236742</td>\n","      <td>0.648148</td>\n","    </tr>\n","    <tr>\n","      <td>2750</td>\n","      <td>0.000100</td>\n","      <td>0.230656</td>\n","      <td>0.717949</td>\n","    </tr>\n","    <tr>\n","      <td>3000</td>\n","      <td>0.000000</td>\n","      <td>0.236766</td>\n","      <td>0.707965</td>\n","    </tr>\n","    <tr>\n","      <td>3250</td>\n","      <td>0.000000</td>\n","      <td>0.240272</td>\n","      <td>0.672727</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results4/checkpoint-1000\n","Configuration saved in ../tmp/results4/checkpoint-1000/config.json\n","Model weights saved in ../tmp/results4/checkpoint-1000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results4/checkpoint-1000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results4/checkpoint-1000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results4/checkpoint-2000\n","Configuration saved in ../tmp/results4/checkpoint-2000/config.json\n","Model weights saved in ../tmp/results4/checkpoint-2000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results4/checkpoint-2000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results4/checkpoint-2000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","Saving model checkpoint to ../tmp/results4/checkpoint-3000\n","Configuration saved in ../tmp/results4/checkpoint-3000/config.json\n","Model weights saved in ../tmp/results4/checkpoint-3000/pytorch_model.bin\n","tokenizer config file saved in ../tmp/results4/checkpoint-3000/tokenizer_config.json\n","Special tokens file saved in ../tmp/results4/checkpoint-3000/special_tokens_map.json\n","The following columns in the evaluation set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Evaluation *****\n","  Num examples = 1051\n","  Batch size = 64\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n","Loading best model from ../tmp/results4/checkpoint-1000 (score: 0.13591623306274414).\n","The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: __index_level_0__, text. If __index_level_0__, text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 1051\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the test set don't have a corresponding argument in `GPT2ForSequenceClassification.forward` and have been ignored: text. If text are not expected by `GPT2ForSequenceClassification.forward`,  you can safely ignore this message.\n","***** Running Prediction *****\n","  Num examples = 3223\n","  Batch size = 64\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}}]},{"cell_type":"code","execution_count":6,"metadata":{"id":"LTuZEogOf5Mr","executionInfo":{"status":"ok","timestamp":1668169105805,"user_tz":-540,"elapsed":3,"user":{"displayName":"å¤§æ©‹å¹¸è¨˜","userId":"14481296101370409019"}}},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPR12VWUlWoRV+Z7tYcDlw/"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8de09ae72a2741c5b9915c4818ca5bc1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b3e08a7f90ea445396fca89bae2f4d18","IPY_MODEL_5dbd2747e7cf4a68a66fb4f7c395af93","IPY_MODEL_3843a5d2d35f4021a1c27f646fc89219"],"layout":"IPY_MODEL_c2cfe2a518f04b25b69408871f7a06fc"}},"b3e08a7f90ea445396fca89bae2f4d18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b01f70974cb4ce5ab0aa3fb9533c9b6","placeholder":"â€‹","style":"IPY_MODEL_aa908149f14e499eb0c7426b2cd04cea","value":" 75%"}},"5dbd2747e7cf4a68a66fb4f7c395af93":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_9f2dd745cbec48cb99360eb4f55ddc5a","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_69a6b20eca6647a087891b7541a39f5e","value":3}},"3843a5d2d35f4021a1c27f646fc89219":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_31f78c6f74834029b065d7607449d961","placeholder":"â€‹","style":"IPY_MODEL_3dabf8a5efb843918b9c1ac573fccbd8","value":" 3/4 [00:00&lt;00:00,  7.63ba/s]"}},"c2cfe2a518f04b25b69408871f7a06fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b01f70974cb4ce5ab0aa3fb9533c9b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa908149f14e499eb0c7426b2cd04cea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9f2dd745cbec48cb99360eb4f55ddc5a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69a6b20eca6647a087891b7541a39f5e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"31f78c6f74834029b065d7607449d961":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dabf8a5efb843918b9c1ac573fccbd8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"607bbd1e6c284ce3b74da489f9c3efd1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_697d0fa5a7ce4c9aaa513429339e9553","IPY_MODEL_6a1ab7b5ed7641aa89218e6e8947ed03","IPY_MODEL_54d3074a645b43cbbe894776c01054c2"],"layout":"IPY_MODEL_938f21c4186545d98704898fb9cf67a3"}},"697d0fa5a7ce4c9aaa513429339e9553":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1baef6b581bc474da92fe0ea7fa702d1","placeholder":"â€‹","style":"IPY_MODEL_6e5d5c34473a4c60844f7340117c1f8a","value":" 83%"}},"6a1ab7b5ed7641aa89218e6e8947ed03":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f1547088e0a473fa1f997b0899bd0f5","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5f4bbc2922ef4dfb81814950e119e009","value":5}},"54d3074a645b43cbbe894776c01054c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e7a89ae6a154dba9818cf0d42bb30b2","placeholder":"â€‹","style":"IPY_MODEL_bd2755cca9d14d7bb2f0182c0aee08bd","value":" 5/6 [00:00&lt;00:00,  7.68ba/s]"}},"938f21c4186545d98704898fb9cf67a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1baef6b581bc474da92fe0ea7fa702d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e5d5c34473a4c60844f7340117c1f8a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1f1547088e0a473fa1f997b0899bd0f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5f4bbc2922ef4dfb81814950e119e009":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0e7a89ae6a154dba9818cf0d42bb30b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd2755cca9d14d7bb2f0182c0aee08bd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"48d6dea9bcc24a8da97511e39c419ddb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_734a7cd49ce648ea87a24aa94095d097","IPY_MODEL_f31565a6254242c1911f7664f724882c","IPY_MODEL_a5987928eebe43f085ebc08ba0770a79"],"layout":"IPY_MODEL_84bb84e193b14fe8a8e27c3558e6b1b3"}},"734a7cd49ce648ea87a24aa94095d097":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_72f04577e8f946eab344debd69ce2fcf","placeholder":"â€‹","style":"IPY_MODEL_36adea7d84d34fc1b7a80b1421fac5e1","value":" 83%"}},"f31565a6254242c1911f7664f724882c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_de72a03db8e14e30a90bd7cae5a63a4e","max":6,"min":0,"orientation":"horizontal","style":"IPY_MODEL_dd47c82f61c14bf8ab3ab44497e96c02","value":5}},"a5987928eebe43f085ebc08ba0770a79":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_533bd5865b79458b8a519011e938048b","placeholder":"â€‹","style":"IPY_MODEL_c2b65ee9400c45fe828889eb001129a7","value":" 5/6 [00:00&lt;00:00,  7.87ba/s]"}},"84bb84e193b14fe8a8e27c3558e6b1b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72f04577e8f946eab344debd69ce2fcf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36adea7d84d34fc1b7a80b1421fac5e1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"de72a03db8e14e30a90bd7cae5a63a4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd47c82f61c14bf8ab3ab44497e96c02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"533bd5865b79458b8a519011e938048b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2b65ee9400c45fe828889eb001129a7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76b11c90c9fc491cbd4675fa8e75b4f6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3d814133feb44f87a83a8f4210fa6b11","IPY_MODEL_593fbb48c56b4495bb5700119a58ac46","IPY_MODEL_5738ba72315d4a5baa0765f5c7d49cdc"],"layout":"IPY_MODEL_254b5afaa1144c63a9d5da190e947b8b"}},"3d814133feb44f87a83a8f4210fa6b11":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1500f82d55840c5b920cf5ecc0a9566","placeholder":"â€‹","style":"IPY_MODEL_556ec94d0f6348378ec90074d5492084","value":" 75%"}},"593fbb48c56b4495bb5700119a58ac46":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_00681fc7e293424d8dbf739d2c37af70","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_254a7d26215c426a9e4f22b4aeb25db0","value":3}},"5738ba72315d4a5baa0765f5c7d49cdc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_412a62a83c184422b8bfb629ba05b305","placeholder":"â€‹","style":"IPY_MODEL_bda988f7fbd041c5bc75172b0b5a16a1","value":" 3/4 [00:00&lt;00:00,  5.05ba/s]"}},"254b5afaa1144c63a9d5da190e947b8b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1500f82d55840c5b920cf5ecc0a9566":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"556ec94d0f6348378ec90074d5492084":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"00681fc7e293424d8dbf739d2c37af70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"254a7d26215c426a9e4f22b4aeb25db0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"412a62a83c184422b8bfb629ba05b305":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bda988f7fbd041c5bc75172b0b5a16a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"485bdc22695b4602a2c44e9eae496fc4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_37f3de8aa0964b23bcbbfff1daa67040","IPY_MODEL_b85bfe1925654bb2918537615ab2ef7d","IPY_MODEL_cf5cc4256653429784cbd9de0e16ac30"],"layout":"IPY_MODEL_c5947e7d26294779973a8be830df7561"}},"37f3de8aa0964b23bcbbfff1daa67040":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f2e903b4df64feb9d71ef18583ae0b1","placeholder":"â€‹","style":"IPY_MODEL_a52d43c746f744359d40eac33dde82c5","value":" 86%"}},"b85bfe1925654bb2918537615ab2ef7d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d1f4cb770374086a1ed41e7f012f766","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_db6d023996db4d5a84dce9fbfe8bbc5a","value":6}},"cf5cc4256653429784cbd9de0e16ac30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5a5ec90df884c3d974dd131c39c1814","placeholder":"â€‹","style":"IPY_MODEL_1fac7d2e3d574bf18d171e85ccf1c72c","value":" 6/7 [00:01&lt;00:00,  5.19ba/s]"}},"c5947e7d26294779973a8be830df7561":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7f2e903b4df64feb9d71ef18583ae0b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a52d43c746f744359d40eac33dde82c5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7d1f4cb770374086a1ed41e7f012f766":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db6d023996db4d5a84dce9fbfe8bbc5a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5a5ec90df884c3d974dd131c39c1814":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1fac7d2e3d574bf18d171e85ccf1c72c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dfa1cca173d447c1899bfd96d5c89391":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_43bc8e3128814fef98f34d2af8ec1352","IPY_MODEL_f0b6e50f5b6540e59016248b344cf0d3","IPY_MODEL_b5efc16760e746138914f4a873c40554"],"layout":"IPY_MODEL_9b63f33ef25244b7b8e5233596c15246"}},"43bc8e3128814fef98f34d2af8ec1352":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0f359969fa62499f8d597cc5bda29d4d","placeholder":"â€‹","style":"IPY_MODEL_47c4eb84d2674f398285426766031f82","value":" 50%"}},"f0b6e50f5b6540e59016248b344cf0d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_b65d6e42cdcd46fe9e0834ba241de6b2","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6b0073fc960d495a8c38dd501cc295de","value":1}},"b5efc16760e746138914f4a873c40554":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3925177bb09c4fb3a8de8a1f1822b262","placeholder":"â€‹","style":"IPY_MODEL_600926c0f2c0456d965761369574dbaf","value":" 1/2 [00:00&lt;00:00,  4.65ba/s]"}},"9b63f33ef25244b7b8e5233596c15246":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f359969fa62499f8d597cc5bda29d4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"47c4eb84d2674f398285426766031f82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b65d6e42cdcd46fe9e0834ba241de6b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b0073fc960d495a8c38dd501cc295de":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3925177bb09c4fb3a8de8a1f1822b262":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"600926c0f2c0456d965761369574dbaf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cef4fa8e86b1418d8389fa1626916bf4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b818bdee18f1482eb532ede4a8658f69","IPY_MODEL_d7ded6850b49496782c3f41bd6c0d91d","IPY_MODEL_1dae50a35f7e4f869a1acb9b37b87972"],"layout":"IPY_MODEL_9a7a52584d804ade96a568be7d6e6221"}},"b818bdee18f1482eb532ede4a8658f69":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e394a2d95b2b4d5a905e050983e38416","placeholder":"â€‹","style":"IPY_MODEL_88a48495ba20489d911e9e3cb64eb78b","value":" 86%"}},"d7ded6850b49496782c3f41bd6c0d91d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_7947126e9b9948d18467727cfc4d6562","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b1fa991106ea41438a4b8c31bf260e58","value":6}},"1dae50a35f7e4f869a1acb9b37b87972":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_181bd894ce7a4c2ab6a9c0eda613ea19","placeholder":"â€‹","style":"IPY_MODEL_711fd0b6a7704f4f9729e0e1b3ebb0f3","value":" 6/7 [00:01&lt;00:00,  5.26ba/s]"}},"9a7a52584d804ade96a568be7d6e6221":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e394a2d95b2b4d5a905e050983e38416":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"88a48495ba20489d911e9e3cb64eb78b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7947126e9b9948d18467727cfc4d6562":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1fa991106ea41438a4b8c31bf260e58":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"181bd894ce7a4c2ab6a9c0eda613ea19":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"711fd0b6a7704f4f9729e0e1b3ebb0f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"39f873659b8c4e6b8b8f26096c892f3c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c07ead273e904c63a70e770539dea2ce","IPY_MODEL_8dff4686cfde47e4ba9ba93404489d11","IPY_MODEL_94639c6b0f234941ad6c8799814500f6"],"layout":"IPY_MODEL_9d48ca5f4de44a328c20ae73e35a350d"}},"c07ead273e904c63a70e770539dea2ce":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8ad7c7792a3472081982402dd478c33","placeholder":"â€‹","style":"IPY_MODEL_43544eb5e3ba40cc9c2feaecf70deac6","value":" 50%"}},"8dff4686cfde47e4ba9ba93404489d11":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_faa39562cb3e4bb2804930afd6368c2c","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c75f6fae6f04340869487fb7adb4659","value":1}},"94639c6b0f234941ad6c8799814500f6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d85efa4c4cd747aaa6f5d4809d6c517b","placeholder":"â€‹","style":"IPY_MODEL_f81398f080794e2c846522ca307f308c","value":" 1/2 [00:00&lt;00:00,  4.59ba/s]"}},"9d48ca5f4de44a328c20ae73e35a350d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8ad7c7792a3472081982402dd478c33":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"43544eb5e3ba40cc9c2feaecf70deac6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"faa39562cb3e4bb2804930afd6368c2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c75f6fae6f04340869487fb7adb4659":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d85efa4c4cd747aaa6f5d4809d6c517b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f81398f080794e2c846522ca307f308c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bdd18eb0417446e7bc4dace5bcfc518a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_41cbf380ab5342a09e92795ee805a730","IPY_MODEL_647d12b2e39240f3bd08ea3e6bb92666","IPY_MODEL_f2ccf14656d74e0b8b0631ba7fe87623"],"layout":"IPY_MODEL_9d215692b2cf4c25921b96ce91c57cc1"}},"41cbf380ab5342a09e92795ee805a730":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d72bbb851919447b96373139f40107ab","placeholder":"â€‹","style":"IPY_MODEL_37cb3a06fb38407a81538d8b30eb1111","value":" 86%"}},"647d12b2e39240f3bd08ea3e6bb92666":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0225dcc0c954b7d9cb9bffe661f1478","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5b7052614d444cebaf39eb41f276018f","value":6}},"f2ccf14656d74e0b8b0631ba7fe87623":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1f70706a10fb4cebb0588242df422c6a","placeholder":"â€‹","style":"IPY_MODEL_1441a62392044d8eb6fc5f238df536cd","value":" 6/7 [00:01&lt;00:00,  5.25ba/s]"}},"9d215692b2cf4c25921b96ce91c57cc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d72bbb851919447b96373139f40107ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"37cb3a06fb38407a81538d8b30eb1111":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f0225dcc0c954b7d9cb9bffe661f1478":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b7052614d444cebaf39eb41f276018f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1f70706a10fb4cebb0588242df422c6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1441a62392044d8eb6fc5f238df536cd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ee07bedbcc00456a8d40a9215d3117d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0ddaffedee88407e972b1caf82655cc0","IPY_MODEL_e613314a96244a199c3943ee714bfea3","IPY_MODEL_788a5532f78f413a84005ae794a0162f"],"layout":"IPY_MODEL_56968a761cd24c94ada12ebf951e26ce"}},"0ddaffedee88407e972b1caf82655cc0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86efacd0003c4eaeb67b6b6ac0b10210","placeholder":"â€‹","style":"IPY_MODEL_373db82d8e4d467bbc4554c2cddf52bb","value":" 50%"}},"e613314a96244a199c3943ee714bfea3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_f29d77eeeceb4302b03e59299fe2eadf","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ff1fc0ba199e4911856f11ebd819f2a2","value":1}},"788a5532f78f413a84005ae794a0162f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_851d2ac043174ca184b5e7757bf741e5","placeholder":"â€‹","style":"IPY_MODEL_e9bbf888b68a4804b58c11c46e85b238","value":" 1/2 [00:00&lt;00:00,  4.60ba/s]"}},"56968a761cd24c94ada12ebf951e26ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86efacd0003c4eaeb67b6b6ac0b10210":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"373db82d8e4d467bbc4554c2cddf52bb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f29d77eeeceb4302b03e59299fe2eadf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff1fc0ba199e4911856f11ebd819f2a2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"851d2ac043174ca184b5e7757bf741e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9bbf888b68a4804b58c11c46e85b238":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e3c63927ea404b63bb770bacb0b18e00":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a0995a4f86a5459e8aad531fbb425e2d","IPY_MODEL_94233c9009554958b9628a711c57c83c","IPY_MODEL_fc86cc9aa0774d79aef8e260636d2013"],"layout":"IPY_MODEL_1b112b9bc1814f8394ded9c9f4f5c56e"}},"a0995a4f86a5459e8aad531fbb425e2d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e6ca2e1575343b4b3de5a2d5db37b26","placeholder":"â€‹","style":"IPY_MODEL_9de91bf760314914a463f49410212f77","value":" 86%"}},"94233c9009554958b9628a711c57c83c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_8dcae2c05ccc46469a3da3d85222dfd5","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_89b873bf17374cb58275e528d06d92ff","value":6}},"fc86cc9aa0774d79aef8e260636d2013":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bcd2c879307947f093b79115cb9b3384","placeholder":"â€‹","style":"IPY_MODEL_0f50419d81364f4382ced9b9e56a13a8","value":" 6/7 [00:01&lt;00:00,  5.23ba/s]"}},"1b112b9bc1814f8394ded9c9f4f5c56e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e6ca2e1575343b4b3de5a2d5db37b26":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9de91bf760314914a463f49410212f77":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8dcae2c05ccc46469a3da3d85222dfd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89b873bf17374cb58275e528d06d92ff":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bcd2c879307947f093b79115cb9b3384":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f50419d81364f4382ced9b9e56a13a8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1496965b5a740d3a38f4f057288e408":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_51fad95e4ffb4076bc7ac021b35fbc37","IPY_MODEL_1a091fd4cce24b41a6a3d6ce0952e845","IPY_MODEL_9177941bba5b475b9c3df3f2d7de25b6"],"layout":"IPY_MODEL_d614a140bf2f465a9700b9562b32303b"}},"51fad95e4ffb4076bc7ac021b35fbc37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5bd9798c3776400ea63d349ef12991a3","placeholder":"â€‹","style":"IPY_MODEL_d486053cf3d94531b9f5473c1244b35f","value":" 50%"}},"1a091fd4cce24b41a6a3d6ce0952e845":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_3d873052513e460ba4d59024b67ced21","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd7b0530da074df6ae11070cacb9b5aa","value":1}},"9177941bba5b475b9c3df3f2d7de25b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_66432dc2ed5d43fda6a6f3b0490e5f01","placeholder":"â€‹","style":"IPY_MODEL_063f4b6978904ad9a2101de9d5135557","value":" 1/2 [00:00&lt;00:00,  4.53ba/s]"}},"d614a140bf2f465a9700b9562b32303b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5bd9798c3776400ea63d349ef12991a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d486053cf3d94531b9f5473c1244b35f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3d873052513e460ba4d59024b67ced21":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd7b0530da074df6ae11070cacb9b5aa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"66432dc2ed5d43fda6a6f3b0490e5f01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"063f4b6978904ad9a2101de9d5135557":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a958737b22db4d37a86ad0bb7e92a529":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_24c36c61a5774661ae4ca1acfdce82f0","IPY_MODEL_c35c41a733fd4824a9c3e8bab3df31ee","IPY_MODEL_b4087c6061714f08a9c6ed059e078216"],"layout":"IPY_MODEL_7668063bdb1347de865284495a5ff0cb"}},"24c36c61a5774661ae4ca1acfdce82f0":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6bbdce098eb49968541f0f7952f3f77","placeholder":"â€‹","style":"IPY_MODEL_2344fb5f20f94305bbeda9549d05c092","value":" 86%"}},"c35c41a733fd4824a9c3e8bab3df31ee":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d68fd48c8034fa0b683ac80eb93af82","max":7,"min":0,"orientation":"horizontal","style":"IPY_MODEL_31801d80f32e4b7db066a49524011300","value":6}},"b4087c6061714f08a9c6ed059e078216":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_246b327472d54d52b9ae1a432671e75c","placeholder":"â€‹","style":"IPY_MODEL_c2b42a7a5c4e49208c0b741db5aef4b9","value":" 6/7 [00:01&lt;00:00,  5.23ba/s]"}},"7668063bdb1347de865284495a5ff0cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6bbdce098eb49968541f0f7952f3f77":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2344fb5f20f94305bbeda9549d05c092":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0d68fd48c8034fa0b683ac80eb93af82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31801d80f32e4b7db066a49524011300":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"246b327472d54d52b9ae1a432671e75c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c2b42a7a5c4e49208c0b741db5aef4b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90e127ea0e8c4946a927b6ef98888ab5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2bc4d56f3027484c8ce1619e3db31fab","IPY_MODEL_d15ca3db1a5448699617a9991f96acd8","IPY_MODEL_b77204317cf442a09201fa171d695c91"],"layout":"IPY_MODEL_bc715eb650234921b168555f40ed0f56"}},"2bc4d56f3027484c8ce1619e3db31fab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf08a9aca4bd4148873ba0450b535e4e","placeholder":"â€‹","style":"IPY_MODEL_97805e70852845c68126a2a07c2c1370","value":" 50%"}},"d15ca3db1a5448699617a9991f96acd8":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b8d86dc8354438b916536385fea0ce0","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_56d9239e3107485e979e7df1023a3c8a","value":1}},"b77204317cf442a09201fa171d695c91":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0931b0def45a4bd98f05d60628a5d345","placeholder":"â€‹","style":"IPY_MODEL_3391c373c5c24bda9776e2de3947e280","value":" 1/2 [00:00&lt;00:00,  4.49ba/s]"}},"bc715eb650234921b168555f40ed0f56":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf08a9aca4bd4148873ba0450b535e4e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97805e70852845c68126a2a07c2c1370":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6b8d86dc8354438b916536385fea0ce0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56d9239e3107485e979e7df1023a3c8a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0931b0def45a4bd98f05d60628a5d345":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3391c373c5c24bda9776e2de3947e280":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}